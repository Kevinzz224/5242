{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:21:56.503977Z",
     "start_time": "2024-03-30T18:21:56.497071Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms as T\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Img preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:21:57.830978Z",
     "start_time": "2024-03-30T18:21:57.821296Z"
    }
   },
   "outputs": [],
   "source": [
    "image_paths = sorted([str(p) for p in glob('img_align_celeba' + '/*.jpg')])\n",
    "print(image_paths.__len__())\n",
    "\n",
    "# 定义数据预处理的transforms\n",
    "image_size = 128\n",
    "\n",
    "# 数据预处理的transforms，将图像大小调整为image_size，并进行标准化\n",
    "transforms = T.Compose([\n",
    "    T.Resize((image_size, image_size), Image.BICUBIC),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # to scale [-1,1] with tanh activation\n",
    "])\n",
    "\n",
    "inverse_transforms = T.Compose([\n",
    "    T.Normalize(-1, 2),\n",
    "    T.ToPILImage()\n",
    "])\n",
    "\n",
    "# 划分训练集、验证集和测试集\n",
    "train, valid = train_test_split(image_paths, test_size=5000, shuffle=True, random_state=42)\n",
    "valid, test = train_test_split(valid, test_size=1000, shuffle=True, random_state=42)\n",
    "\n",
    "# 输出数据集长度\n",
    "print(f'Train size: {len(train)}, validation size: {len(valid)}, test size: {len(test)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:21:59.651334Z",
     "start_time": "2024-03-30T18:21:59.645572Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 16\n",
    "lr = 8e-5\n",
    "mask_size = 64\n",
    "path = r'painting_model.pth'\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "patch_h, patch_w = int(mask_size / 2 ** 3), int(mask_size / 2 ** 3)\n",
    "patch = (1, patch_h, patch_w)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "## cuda信息\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:00.596682Z",
     "start_time": "2024-03-30T18:22:00.582702Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CelebaDataset(Dataset):\n",
    "    def __init__(self, images_paths, transforms=transforms, train=True):\n",
    "        self.images_paths = images_paths\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def apply_center_mask(self, image):\n",
    "        # 将mask应用于图像的中心部分//遮挡中心部分\n",
    "        idx = (image_size - mask_size) // 2\n",
    "        masked_image = image.clone()\n",
    "        masked_image[:, idx:idx+mask_size, idx:idx+mask_size] = 1\n",
    "        masked_part = image[:, idx:idx+mask_size, idx:idx+mask_size]\n",
    "        return masked_image, idx\n",
    "    \n",
    "    def apply_random_mask(self, image):\n",
    "        # 将mask随机应用于图像的某个区域\n",
    "        y1, x1 = np.random.randint(0, image_size-mask_size, 2)\n",
    "        y2, x2 = y1 + mask_size, x1 + mask_size\n",
    "        masked_part = image[:, y1:y2, x1:x2]\n",
    "        masked_image = image.clone()\n",
    "        masked_image[:, y1:y2, x1:x2] = 1\n",
    "        return masked_image, masked_part\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        path = self.images_paths[ix]\n",
    "        image = Image.open(path)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        if self.train:\n",
    "            masked_image, masked_part = self.apply_random_mask(image)\n",
    "        else:\n",
    "            masked_image, masked_part = self.apply_center_mask(image)\n",
    "            \n",
    "        return image, masked_image, masked_part\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, masked_images, masked_parts = list(zip(*batch))\n",
    "        images, masked_images, masked_parts = [[tensor.to(device) for tensor in tensors] for tensors in [images, masked_images, masked_parts]]\n",
    "        #images, masked_images, masked_parts = [torch.cat(ims) for ims in [images, masked_images, masked_parts]]\n",
    "        images= torch.cat(images).view(batch_size, -1, image_size, image_size)\n",
    "        masked_images = torch.cat(masked_images).view(batch_size, -1, image_size, image_size)\n",
    "        masked_parts = torch.cat(masked_parts).view(batch_size, -1, mask_size, mask_size)\n",
    "        return images, masked_images, masked_parts\n",
    "        \n",
    " # 创建数据集和数据加载器\n",
    "train_dataset = CelebaDataset(train)\n",
    "valid_dataset = CelebaDataset(valid, train=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:01.890168Z",
     "start_time": "2024-03-30T18:22:01.885114Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight, 0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, 1, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def set_params(model, unfreeze):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = unfreeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#定义生成器网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:02.633273Z",
     "start_time": "2024-03-30T18:22:02.617682Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResDown(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, scale=2):\n",
    "        super(ResDown, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out//2, 3, 1, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(channel_out//2, 0.8)\n",
    "        self.conv2 = nn.Conv2d(channel_out//2, channel_out, 3, scale, 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(channel_out, 0.8)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, 3, scale, 1)\n",
    "        self.activation = nn.LeakyReLU(0.2)  # 激活层\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.conv3(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x + skip)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUp(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, scale=2):\n",
    "        super(ResUp, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out//2, 3, 1, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(channel_out//2, 0.8)\n",
    "        self.conv2 = nn.Conv2d(channel_out//2, channel_out, 3, 1, 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(channel_out, 0.8)\n",
    "\n",
    "        self.upscale = nn.Upsample(scale_factor=scale, mode=\"nearest\")  # 上采样层\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, 3, 1, 1)\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.conv3(self.upscale(x))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(self.upscale(x))\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        x = self.activation(x + skip)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:03.376671Z",
     "start_time": "2024-03-30T18:22:03.365193Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  # 编码器\n",
    "\n",
    "    def __init__(self, channels, ch=64, z=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ResDown(channels, ch)\n",
    "        self.conv2 = ResDown(ch, 2*ch)\n",
    "        self.conv3 = ResDown(2*ch, 4*ch)\n",
    "        self.conv4 = ResDown(4*ch, 8*ch)\n",
    "        self.conv5 = ResDown(8*ch, 8*ch)\n",
    "        self.conv_mu = nn.Conv2d(8*ch, z, 2, 2)  # 卷积层\n",
    "        self.conv_log_var = nn.Conv2d(8*ch, z, 2, 2)\n",
    "\n",
    "    def sample(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        mu = self.conv_mu(x)\n",
    "        log_var = self.conv_log_var(x)\n",
    "        x = self.sample(mu, log_var)\n",
    "\n",
    "        return x, mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):  # 解码器\n",
    "\n",
    "    def __init__(self, channels, ch=64, z=512):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = ResUp(z, ch*8)\n",
    "        self.conv2 = ResUp(ch*8, ch*4)\n",
    "        self.conv3 = ResUp(ch*4, ch*2)\n",
    "        self.conv4 = ResUp(ch*2, ch)\n",
    "        self.conv5 = ResUp(ch, ch//2)\n",
    "        self.conv6 = nn.Conv2d(ch//2, channels, 3, 1, 1)  # 卷积池\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):  # 传播的是解码器解码后的特征图\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:04.131007Z",
     "start_time": "2024-03-30T18:22:04.124344Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, channel_in=3, ch=64, z=512):\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):  # 传播编码器编码后的特征向量\n",
    "        encoding, mu, log_var = self.encoder(x)\n",
    "        recon = self.decoder(encoding)\n",
    "        return recon, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判别器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:04.813139Z",
     "start_time": "2024-03-30T18:22:04.801909Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize, dropout, spectral):\n",
    "            if spectral:  # 封装稳定训练\n",
    "                layers = [nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(in_filters, out_filters, 3, stride, 1), n_power_iterations=2)]\n",
    "            else:\n",
    "                layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:  # 归一化\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            if dropout:  # 防止过拟合\n",
    "                layers.append(nn.Dropout(p=0.5))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize, dropout, spectral in [(64, 2, False, 0, 0), (128, 2, True, 0, 0), (256, 2, True, 0, 0), (512, 1, True, 0, 0)]:\n",
    "            layers.extend(discriminator_block(\n",
    "                in_filters, out_filters, stride, normalize, dropout, spectral))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:05.558784Z",
     "start_time": "2024-03-30T18:22:05.193660Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "generator = ResnetGenerator().apply(init_weights).to(device) \n",
    "summary(generator, (3, 128, 128)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:06.375316Z",
     "start_time": "2024-03-30T18:22:06.342282Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator().apply(init_weights).to(device)\n",
    "summary(discriminator, (3, 64, 64)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义对抗损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:09.534080Z",
     "start_time": "2024-03-30T18:22:09.526461Z"
    }
   },
   "outputs": [],
   "source": [
    "class KLDLoss(nn.Module):\n",
    "    def forward(self, mu, logvar, beta=1.0):\n",
    "        kld = -0.5 * torch.sum(1 + logvar -\n",
    "                               torch.pow(mu, 2) - torch.exp(logvar))\n",
    "        return beta * kld\n",
    "\n",
    "\n",
    "reconstruction_loss = nn.functional.mse_loss\n",
    "kld_loss = KLDLoss()\n",
    "\n",
    "\n",
    "def kld_criterion(x, y, mu, logvar): \n",
    "    #return reconstruction_loss(\n",
    "    #y, x, reduction=\"sum\") + kld_loss(mu, logvar, beta=0.1)\n",
    "    a = reconstruction_loss(y, x, reduction='sum')\n",
    "    b = kld_loss(mu, logvar, beta = 0.1)\n",
    "    return a + b\n",
    "\n",
    "adversarial_loss = nn.MSELoss()  # 对抗损失，使用均方误差损失代替二进制交叉熵损失\n",
    "\n",
    "# 优化器\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    generator.parameters(), lr=lr, betas=(b1, b2))  # 生成器优化器\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(b1, b2))  # 判别器优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:22:11.782837Z",
     "start_time": "2024-03-30T18:22:11.774992Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_one_batch(batch, generator, discriminator, criterion_adv, criterion_pix, optimizer_G, optimizer_D):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    images, masked_images, masked_parts = batch\n",
    "    real = torch.FloatTensor(batch_size, *patch).fill_(1.0).requires_grad_(False).to(device)  # 真实样本标签\n",
    "    fake = torch.FloatTensor(batch_size, *patch).fill_(0.0).requires_grad_(False).to(device)  # 生成样本标签\n",
    "    \n",
    "    set_params(discriminator, False) \n",
    "    optimizer_G.zero_grad() \n",
    "    gen_parts, mu, logvar = generator(masked_images) \n",
    "    \n",
    "    gan_loss = criterion_adv(discriminator(gen_parts), real) \n",
    "    pix_loss = criterion_pix(masked_parts, gen_parts, mu, logvar) \n",
    "    \n",
    "    loss_g = 0.001 * gan_loss + 0.999 * pix_loss \n",
    "    loss_g.backward() \n",
    "    optimizer_G.step() \n",
    "    \n",
    "    set_params(discriminator, True) \n",
    "    optimizer_D.zero_grad() \n",
    "\n",
    "    real_loss = criterion_adv(discriminator(masked_parts), real) # 真实样本损失\n",
    "    fake_loss = criterion_adv(discriminator(gen_parts.detach()), fake) # 生成样本损失\n",
    "    \n",
    "    loss_d = (real_loss + fake_loss) / 2  \n",
    "    loss_d.backward()\n",
    "    optimizer_D.step() \n",
    "    \n",
    "    return loss_g.item(), loss_d.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:28:29.977075Z",
     "start_time": "2024-03-30T18:28:29.964875Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_one_batch(batch, generator, discriminator, criterion_adv, criterion_pix):\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    images, masked_images, masked_parts = batch\n",
    "    real = torch.FloatTensor(batch_size, *patch).fill_(1.0).requires_grad_(False).to(device) # 真实样本标签\n",
    "    fake = torch.FloatTensor(batch_size, *patch).fill_(0.0).requires_grad_(False).to(device) # 生成样本标签\n",
    "    \n",
    "    gen_parts, mu, logvar = generator(masked_images)\n",
    "    \n",
    "    gan_loss = criterion_adv(discriminator(gen_parts), real)\n",
    "    pix_loss = criterion_pix(masked_parts, gen_parts, mu, logvar) \n",
    "    \n",
    "    loss_g = 0.001 * gan_loss + 0.999 * pix_loss\n",
    "    \n",
    "    real_loss = criterion_adv(discriminator(masked_parts), real)\n",
    "    fake_loss = criterion_adv(discriminator(gen_parts.detach()), fake)\n",
    "    \n",
    "    loss_d = (real_loss + fake_loss) / 2 \n",
    "    return loss_g, loss_d\n",
    "     \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_plot(test, generator, scale=1):\n",
    "    idx = np.random.randint(len(test))\n",
    "    random_path = test[idx]\n",
    "    \n",
    "    image = Image.open(random_path)\n",
    "    image = transforms(image)\n",
    "    \n",
    "    masked_image, idx = train_dataset.apply_center_mask(image)\n",
    "    \n",
    "    generator.eval()\n",
    "    gen_part = generator(masked_image.unsqueeze(0).to(device))[0].squeeze(0).cpu().detach()\n",
    "    gen_image = masked_image.clone()\n",
    "    gen_image[:, idx:idx+mask_size, idx:idx+mask_size] = gen_part\n",
    "    \n",
    "    # scale [-1,1] or [0,1]\n",
    "    if scale:\n",
    "        run_transforms = inverse_transforms\n",
    "    else:\n",
    "        run_transforms = T.ToPILImage()\n",
    "    image = run_transforms(image)\n",
    "    masked_image = run_transforms(masked_image)\n",
    "    gen_image = run_transforms(gen_image)\n",
    "    \n",
    "    #生成对比图片\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.title('Masked Image')\n",
    "    plt.imshow(masked_image)\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.title('Inpainted Image')\n",
    "    plt.imshow(gen_image)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T18:39:34.005097Z",
     "start_time": "2024-03-30T18:37:50.794095Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#初始化损失\n",
    "train_d_losses, valid_d_losses = [], []\n",
    "train_g_losses, valid_g_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    # 训练集迭代\n",
    "    tq_bar = tqdm(train_dataloader, total=len(train_dataloader), desc=f'Train step {epoch+1}')\n",
    "    epoch_d_losses, epoch_g_losses = [], []\n",
    "    for _, batch in enumerate(tq_bar):\n",
    "        g_loss, d_loss = train_one_batch(batch, generator, discriminator, adversarial_loss, \n",
    "                                         kld_criterion, optimizer_G, optimizer_D)\n",
    "        epoch_g_losses.append(g_loss)\n",
    "        epoch_d_losses.append(d_loss)\n",
    "        tq_bar.set_postfix(g_loss=np.mean(epoch_g_losses), d_loss=np.mean(epoch_d_losses))\n",
    "    \n",
    "    train_d_losses.append(np.mean(epoch_d_losses))\n",
    "    train_g_losses.append(np.mean(epoch_g_losses))\n",
    "    # 验证集迭代\n",
    "    tq_bar = tqdm(valid_dataloader, total=len(valid_dataloader), desc=f'Validation step {epoch+1}')\n",
    "    epoch_d_losses, epoch_g_losses = [], []\n",
    "    for _, batch in enumerate(tq_bar):\n",
    "        g_loss, d_loss = validate_one_batch(batch, generator, discriminator, adversarial_loss, kld_criterion)\n",
    "        epoch_d_losses.append(d_loss.detach().cpu().numpy())\n",
    "        epoch_g_losses.append(g_loss.detach().cpu().numpy())\n",
    "        tq_bar.set_postfix(g_loss=np.mean(epoch_g_losses), d_loss=np.mean(epoch_d_losses))\n",
    "\n",
    "    valid_d_losses.append(np.mean(epoch_d_losses))\n",
    "    valid_g_losses.append(np.mean(epoch_g_losses))\n",
    "    \n",
    "    if (epoch+1) % 2 == 0 or (epoch+1) == epochs:\n",
    "        test_plot(test, generator)\n",
    "        checkpoint = {\n",
    "            'discriminator': discriminator,\n",
    "            'generator': generator,\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T17:39:59.229204Z",
     "start_time": "2024-03-30T17:39:59.224048Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"painting_model.pth\")\n",
    "discriminator = checkpoint['discriminator']\n",
    "generator = checkpoint['generator']\n",
    "\n",
    "# 准备要修复的图像\n",
    "image_path = \"demo.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image = transforms(image)\n",
    "\n",
    "# 对图像进行遮盖，生成待修复的图像\n",
    "masked_image, idx = train_dataset.apply_center_mask(image)\n",
    "\n",
    "# 使用生成器对遮盖后的图像进行修复\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    gen_part = generator(masked_image.unsqueeze(0).to(device))[0].squeeze(0).cpu().detach()\n",
    "    gen_image = masked_image.clone()\n",
    "    gen_image[:, idx:idx+mask_size, idx:idx+mask_size] = gen_part\n",
    "\n",
    "# 将修复后的图像转换回原始格式\n",
    "gen_image = inverse_transforms(gen_image)\n",
    "masked_image = inverse_transforms(masked_image)\n",
    "image = inverse_transforms(image)\n",
    "\n",
    "# 显示原图、mask 图和输出图\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(masked_image)\n",
    "plt.title(\"Masked Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(gen_image)\n",
    "plt.title(\"Inpainted Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
